---
title: "Stress-testing Phea, test B"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Stress testing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  _This vignette assumes a SQL server at `localhost` (we use PostgreSQL), with data in OMOP Common Data Model v5.4 format in schema `cdm_new_york3`. The patient records shown in this example are synthetic data from [Synthea<sup>TM</sup> Patient Generator](https://github.com/synthetichealth/synthea)._  
```{r, results = 'hide', include = FALSE}
library(knitr)
options(scipen = 5e5)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
source(paste0(Sys.getenv("HOME"), '/Estudo/Tecnologia/R/espinha/credenciais.R')) # contains cred$pg
library(wrap)

usn <- function(n, decimals = 0) {
  sapply(n, function(e) {
    if(is.na(e))
      return(e)
    # usn()'s decimals behave like round()'s digits, but comma()'s precision is different.
    scales::comma(e, 10^(-decimals))
  })
}
```
```{r setup, message = FALSE}
library(phea)
library(dplyr)

# Connect to SQL server.
dbcon <- DBI::dbConnect(RPostgres::Postgres(),
  host = 'localhost', port = 7654, dbname = 'fort',
  user = cred$pg$user, password = cred$pg$pass)

# Call setup_phea so we can use sqlt() and sql0().
setup_phea(dbcon, 'cdm_new_york3')
```

How complex can a phenotype be in [Phea](https://github.com/fabkury/phea)? And how does performance change as you increase the complexity? Do the queries ever become **too complex**?  

Especifically, we talk about two dimensions of complexity:

> A. How many components you can put into one formula.  
> B. How many _consecutive_ times can you repeat this: compute a formula, use its results as a component inside another formula.  
 
In this vignette we stress-test Phea in dimension *B*. We are calling this _stress test B_: 

> a. Compute a formula, use the result in a new formula, repeat 1 to 150 times.  
 
The phenotype we compute here is not meaningful in a real-world sense. We just want to test Phea. At the end the results help us see what is going on at technical level.  

## TL;DR

## The dataset  
We use schema `cdm_new_york3`. It has data generated by [Synthea<sup>TM</sup> Patient Generator](https://github.com/synthetichealth/synthea), ETL'ed to OMOP Common Data Model version 5.4 by the grace of and credit to [ETL-Synthea](https://github.com/OHDSI/ETL-Synthea).  

For context, let us glance at the size of that CDM instance:    

```{r size_stats}
persons <- sqlt(person) |>
  summarise(persons = n_distinct(person_id)) |>
  pull('persons')
```

Table `PERSON` has **`r usn(as.numeric(persons))` person IDs.** 

Let's see the most popular concepts in it.  
```{r measurement_summary_peek}
# head(measurement_summary, 10) |>
#   kable()
```

That is the data we will use to benchmark Phea. Each component will be exactly like this:  
```sql
SELECT *
FROM cdm_new_york3.measurement
WHERE measurement_concept_id = [chosen concept]
```
In other words, each component uses a different `measurement_concept_id`, from the summary table, to `SELECT` from `MEASUREMENT`.  
  
We will be picking rows (concepts) from the summary table starting from the first, so, to reduce bias, let us shuffle them.  
```{r}
# Shuffle the rows
# set.seed(42)
# measurement_summary <- measurement_summary[sample(1:nrow(measurement_summary)),]
```

## Computer used to run this test
This was the machine used to execute this test:  

> Processor	Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz   2.59 GHz  
> Installed RAM	64.0 GB (63.8 GB usable)  
> System type	64-bit operating system, x64-based processor  

## Formulas used for testing
For each run of test B, we will take the result of the prior formula and add a random number to it. In the first iteration, the prior number is 0.  

> **test_b<sub>1</sub> = 0.0**
> **test_b<sub>n+1</sub> = test_b<sub>n</sub> + random()**

In other words, we'll perform a random walk on a 1-dimensional space.  

All persons extant in table `MEASUREMENT` will participate. Each person will be running its own random walk.  

In each run, we use library `tictoc` to measure the time spent.    

```{r}
library(tictoc)
```

We will be using table `MEASUREMENT` from `cdm_new_york3`. For context, let us measure the size of that table.  

```{r size_stats_noeval, eval = FALSE}
size_stats <- sqlt(measurement) |>
  group_by() |>
  summarise(
    rows = n(),
    patients = n_distinct(person_id)) |>
  collect()

measurement_summary <- sql0('
    select measurement_concept_id as concept_id, concept_code, concept_name, vocabulary_id,
      count(*) as records, count(distinct person_id) as patients
    from cdm_new_york3.measurement a
    inner join cdm_new_york3.concept b
    on a.measurement_concept_id = b.concept_id
    group by measurement_concept_id, concept_code, concept_name, vocabulary_id
    order by patients desc, records desc
  ') |>
  collect()
```

```{r size_stats, echo = FALSE, results = 'hide', message = FALSE}
size_stats <- sqlt(measurement) |>
  group_by() |>
  summarise(
    rows = n(),
    patients = n_distinct(person_id)) |>
  collect()

wrap(measurement_summary, {
  sql0('
    select measurement_concept_id as concept_id, concept_code, concept_name, vocabulary_id,
      count(*) as records, count(distinct person_id) as patients
    from cdm_new_york3.measurement a
    inner join cdm_new_york3.concept b
    on a.measurement_concept_id = b.concept_id
    group by measurement_concept_id, concept_code, concept_name, vocabulary_id
    order by patients desc, records desc') |>
  collect()
})
```

`MEASUREMENT` has **`r usn(as.numeric(size_stats$rows))` rows** and **`r usn(as.numeric(size_stats$patients))` patients**. Let's see the most popular concepts in it.  

```{r measurement_summary_peek}
head(measurement_summary, 10) |>
  kable()
```

In other words, each component uses a different `measurement_concept_id`, from the summary table, to `SELECT` from `MEASUREMENT`.  
  
We will be picking rows (concepts) from the summary table starting from the first, so, to reduce bias, let us shuffle them.  
```{r}
# Shuffle the rows
set.seed(42)
measurement_summary <- measurement_summary[sample(1:nrow(measurement_summary)),]
```

## Functions used for the test  
Function `produce_N_components(N)` produces `N` components using the first `N` rows of `measurement_summary`.  
```{r}
produce_N_components <- function(N, offset = 0) {
  make_test_component <- function(i) {
    concept_id <- measurement_summary$concept_id[i]
    records <- sqlt(measurement) |>
      filter(measurement_concept_id == local(concept_id))
    component <- make_component(records,
      .ts = measurement_datetime,
      .pid = person_id)
    return(component)
  }
  
  selected <- (1+offset):(N+offset)
  selected <- (selected-1) %% nrow(measurement_summary) + 1 # Cycle if past end
  
  # Pick the selected lab tests
  components <- lapply(selected, make_test_component)
  
  # Their names will be simply "C<sub>n</sub>"
  names(components) <- paste0('c', selected)
  
  return(components)
}
```


Function `run_test_b(N)` runs Test B `N` consecute times.  
```{r}

iterate_test_b <- function(prior_result, i, n_components) {
  new_components <- produce_N_components(n_components, i)
  
  new_components_string <- paste0('coalesce(', names(new_components), '_value_as_number, 0)', collapse = ' + ')
  test_b_formula <- paste0('prior_test_b + rand_value + ', new_components_string)
  
  prior_test_b <- make_component(prior_result, .pid = pid, .ts = ts)
  
  new_result <- calculate_formula(
    components = c(list(prior = prior_test_b), new_components),
    fml = list(
      rand_value = 'random()',
      test_b = test_b_formula),
    .kco = TRUE)
  
  new_result
}


run_test_b <- function(N) {
  n_extra_components <- 4
  paste0('phea_test_b_c', n_extra_components, '_', N) |> wrap(ovr = T, by_name = TRUE, assign_val = FALSE, pass_val = TRUE, {
    # Gather all persons from table PERSON.
    phenotype <- sqlt(person) |>
      transmute(
        # Names must match output from calculate_formula().
        pid = person_id,
        ts = birth_datetime,
        test_b = 0)
    
    tic()
    for(i in 1:N)
      phenotype <- iterate_test_b(phenotype, (i-1)*n_extra_components + 1, n_extra_components)
    phea_time <- toc(quiet = TRUE)
    phea_time <- phea_time$toc - phea_time$tic
    
    tic()
    aggregate <- phenotype |>
      group_by() |>
      summarise(
        rows = n(),
        patients = n_distinct(pid),
        test_b_sum = sum(test_b/1000, na.rm = TRUE)) |>
      collect() |>
      mutate(`N` = local(N))
    query_time <- toc(quiet = TRUE)
    query_time <- query_time$toc - query_time$tic
    
    # phenotype_head
      
    res <- list(
      N = N,
      # result = phenotype,
      aggregate = aggregate,
      phea_time = phea_time,
      query_time = query_time)
    
    res
  })
}

```

Function `print_test_a_results()` will also be used. For brevity it is not printed here. You can see it by downloading the markdown file `stress_test_a.Rmd`.  
```{r, echo = FALSE}

```

## Run N = 1  
```{r}
test_b_runs = list()
```

Let's first run Test A with N = 1, that is, a single component.
```{r test_a_1, results = 'hide', message = FALSE}
test_b_runs[[length(test_b_runs)+1]] <- run_test_b(1)
```

Done! Now let us briefly peek at the results of the phenotype itself.  
```{r test_a_1_peek}
# test_b_runs[[length(test_b_runs)]]$result |>
  # kable()
```
Column `test_a` is the result of the formula, i.e. the sum of the components. As it appears, there are many NAs in `value_as_number` in our table `MEASUREMENT`, but that is not an issue.  
  
How was the performance? Let's see.  
```{r}
# print_test_a_results()
```
Here are the meanings of the columns:  

 - **N**: Number of components included in the run.  
 - **in. rows**: Total number of rows in the record sources provided to `calculate_formula()`.	 
 - **res. rows**: Total number of rows in the phenotype returned by `calculate_formula()`.  
 - **test_a_sum**: Sum of the result of the formula across all patients.  
 - **in. patients**: Number of patients in the record sources.    
 - **res. patients**: Number of patients in the resulting phenotype.   
 - **Phea time**: Time spent by `calculate_formula()`, in seconds.  
 - **query time**: Time spent to run (`collect()`) the phenotype (aggregated), in seconds.   
 - **query pps**: Input patients processed per second of query time.  
 - **query rps**: Input rows processed per second of query time.  

## Run N = 3  
```{r test_b_3, results = 'hide', message = FALSE}
test_b_runs[[length(test_b_runs)+1]] <- run_test_b(5)

# print_test_a_results()
```
```{r, echo = FALSE}
# print_test_a_results()
```
With N = 5 (five components), Phea took `r test_a_runs[[2]]$phea_time` sec to assemble the phenotype, and the SQL server took `r test_a_runs[[2]]$query_time` sec to run the query. That is `r usn(test_a_runs[[2]]$rps_query)` input rows per second of query time.  

## Runs N = 10 to N = 150    
Now that we have a grip on how Test A works, let's run it for N = 10 to 150, increasing 10 at a time.
```{r test_a_10_150}
N <- 5
N_max <- 90 #150
while(N <= N_max) {
  message('Test B, N = ', N, '.')
  test_b_runs[[length(test_b_runs)+1]] <- run_test_b(N)
  N <- N + 5
}
```

## Test B results

### Author contact  
FabrÃ­cio Kury  
`r format(Sys.time(), '%Y/%b/%d')`  
Be always welcome to reach me at fab@kury.dev.  
```{r, include = FALSE}
DBI::dbDisconnect(dbcon)
```
